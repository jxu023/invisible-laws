dont' quite have a good name/title for the proj yet, should ask teammembers to think about it too

had an idea for a regex generator in the style of the excel flashfill synthesis paper

we have have it be a "general" regex generator. Given a set of execution paths of code

look through the code sequences of those paths, generate a "general" regex that would
MATCH on a majority of them or perhaps all of them.

could call it a "fuzzy" or approximate regex generator
    ***** https://laurikari.net/tre/about/ *****
we coudl then use this regex as a candidate rule
it would be a limited form of regex though to be practical i think
again, look at the excel flashfill paper and perhaps some of the other synthesis paper
for reference and examples

The other idea was to use LSA to analyze the source code to generate candidate <b> then <a> rules. I think LSA is able to rank related keywords in portions of code, lets say a document is a function or something like that. We can use the rankings from the LSA thing as input into the approximate regex generator

We coudl at least investigate the possibility of using an approximate regex generator. try to find some papers on this

also find some papers on LSA source code analysis, see how other people have used

finally, the given goal was to be able to automatically or systematically generate a code checker for code that calls some API. try to find other papers related to this and think about related ideas.

look up papers on:
    assertion generation
    lsa for source code analysis
    approximate regex generation
    checking code which calls an api / or implements an api
        example: orig paper checks
            different implementations of common filesystem interface
            device drivers implementation of interrupts

tools:
    clang static analyzer
    llvm pass

approximate regex generator might not be the exact thing i was looking for. The usage of it i had in mind is more like a trend analyzer. Given a perhaps large set of "related" execution paths, derive a very "general" regular expression that matches on most of the paths.

what if this "fuzzy regex fitting" could also be "fuzzy grammar fitting"? would it help the inference tool? make it more powerful? allow it more suggestions?

most languages are already cfg expressible, i guess cfg could be considered as having an element of time too. This case is somewhat different from the classic language cfg in that now we're considering execution path cfg. The rules shoudl apply to both of them, but execution path cfg can be more limited, it can be "straight line code" for simpler checkers can be used

we can divide the project into several parts
    internal consistency
        infer must beliefs
            direct observation
            pre and post condition
        (any optimizations over the paper?)
    code relator (common _____)
        execution path
        interface
        semantic boundary
        infer code belief across versions
        group code into equivalence classes, maybe LSA could be used here
    statistical analysis
        (on top of internal consistency, is there a better way to do this than the paper? lots of things in stats.. pgm, mcmc, need some weighted controls to favour things like local errors and such as they did in paper)
        the paper also mentions augmenting this with "code trustworthiness"
            need to segment code and evaluate "trustworthiness"
            example: old code (more trustworthy) than new code in a proj
                maybe? maybe not?
        inversion ranking
        non-spurious principle
            properties must be true for at least one element
            immediately promote may to must
        handling noise
            "there are so many paths", derive patterns, use promising ones
                aka more data
            rank error messages (from assuming MUST)
                don't rank beliefs (what did paper mean by this exactly?)
        latent specifications
            humans have specific naming conventions, datatypes, special function calls
                this is the place where LSA could be used
    candidate rule generators
        (using LSA, fuzzy regex/grammar fitting on str8 line code).

what other things can we encode about program state/belief from static analysis?

what if we could directly feed something like a boolean expression to a solver for creating our belief set?

beliefs can propagate both forward and backward, this can mean that our implementation of internal consistency is going to be a bit tricky. Well, maybe not relaly. inference of beliefs will use direct obs, pre cond, post cond. only pre cond is backwards, just collect the beliefs without flagging error is all. or maybe collect all inferred beliefs in multiple passes, flag errors in one final pass

everything declared is used at least once ..

threshold for redundancy and contradiction errors: checks separated arbitrary by ~10 executable lines at most (orig paper)

make sure to check out other papers from the orig paper's authors
make sure everyone understands everything in the orig paper
look up papers that cite orig paper and which are cited by orig paper

order can be irrelevant as in the autoises paper
access order consideration can be more expensive too

// LECTURE ********************************************************

every group is going to get feedback, that's a good thing, and for every assignment too? email..

bugs as deviant behaviour - dawnson engler
detect bugs in large scale systems
the problem is: if you want to detect a bug, how do you know what a bug is? (philosophical o_o)

type system
    very "basic" error detector
    C has only a few types
        char, int, short, pointer, float, array, struct
        a very limited, primitive type system
    some language extend on that, gives ability to catch more errors
        tainted pointers, untainted pointers - enabled by type system
    CCured - used type system to catch spatial memory errors
        specified pointers of diff types, became fat pointers
        basically modified the language itself
formal methods
    write a specification
        properties that want to check
            safety
                this property should never be violated
                "don't do anything bad"
            liveness
                this should eventually happen, eventually be used
                "do something good" - "have an effect/action/result"
                security research doesn't really care much about liveness
                useful for other areas
        preconditions, postconditions
            aka requirements, consequences
        needs to be manually written
            # of bugs captured highly depends on well written specification
    it's liked because of the fundamental guarantee of "bug-free"ness
    there's a few bad things though
        mainly that need to give specification
        scalability, large model takes long time to give any results
    there are some improvements in mechanical proof systems
        Coq
            help to check if the logic is right or not
    interesting area
high level compilations
    hardwired application level information
        certain rules embedded - "if you see this then there is error"

problem for all of the above approaches
    you need to know what the rules are
    what is right? what is wrong? what is a bug? what is intended?
    a lot of times you don't have this
    but the rules are often embedded in the code
    just need to finda  way to extract them
    this is what the paper proposes and "very useful"

how to construct rules; basically there is two types/ways
    internal consistency
        try to infer the state or belief based on the code operations
        e.g. null pointer dereference bug
        assignemnt
            becomes null or not null
            precondition is diff from post condition
        comparison
            adds to belief set within the if scope and else scope
        when we have a pointer, it must have one kind of state or belief
            it cannot have two, can only be in one state
        (numBugs numFalsePos)
        check then use(79 26), use then check (102 4)
            check implies belief, use changes state
                each have their own pre and post conditions
        redundant check (24 10)
    templates
        <a> paired with <b>
        function <f> must be checked for error
        no <a> after <b>
        <b> must follow <a>
        in context <c>, do <b> after <a>
    statistical analysis
        build candidates from templates
    

static analysis usually doesn't consider paths, so this can come up with false pos o__o

